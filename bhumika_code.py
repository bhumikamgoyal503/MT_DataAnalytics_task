# -*- coding: utf-8 -*-
"""bhumika_23MI10014_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKmYK__yHFgdoLSXeg-jsOVMaoY60yDF
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

#Read the provided Excel file containing Vendor Information and Job Requirement.
#Inspect the structure to understand the column names and data.
xls = pd.ExcelFile("data.xlsx")  # Load the Excel file
df_vendors = pd.read_excel(xls, sheet_name=0)  # Read the first sheet - vendor IDs
df_jobs = pd.read_excel(xls, sheet_name=1)  # Read the second sheet - Job requirment

#Convert text to lowercase.
#Remove punctuation and special characters.
#Remove stopwords (like "the", "and", "is").
#Perform lemmatization (reduce words to base form, e.g., "running" â†’ "run").

# Download necessary NLTK data #Step 2: Data Preprocessing
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer and stop words
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):  # Ensure text is not NaN
        text = text.lower()  # Convert to lowercase
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
        words = text.split()  # Tokenize
        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatize & remove stopwords
        return " ".join(words)
    return ""

# Apply preprocessing
df_vendors["Cleaned_Scope"] = df_vendors["Detailed Scope"].apply(preprocess_text) #Stores the cleaned text in a new column called "Cleaned_Scope"
df_jobs["Cleaned_Job"] = df_jobs["Job Description"].apply(preprocess_text) #Stores the cleaned text in a new column called "Cleaned_Job".

# Display cleaned data
print(df_vendors.head())
print(df_jobs.head())

print(df_vendors.head()) # head displays first few rows to only observe data and quickly inspect dataset.
print(df_jobs.head())

#Convert text into numerical vectors using TF-IDF.
#Compute Cosine Similarity between vendor scope and job requirement.
#Rank vendors based on similarity.
#Step 3: Compute Text Similarity

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Combine vendor and job description text for vectorization
all_texts = df_vendors["Cleaned_Scope"].tolist() + [df_jobs["Cleaned_Job"][0]]

# Convert text into numerical representation (TF-IDF)
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(all_texts)

# Compute Cosine Similarity between vendors and job requirement
similarity_scores = cosine_similarity(tfidf_matrix[:-1], tfidf_matrix[-1])

# Add similarity scores to DataFrame
df_vendors["Similarity_Score"] = similarity_scores.flatten()

# Display vendors sorted by similarity
df_vendors_sorted = df_vendors.sort_values(by="Similarity_Score", ascending=False)
print(df_vendors_sorted.head(10))

# Select top 10 vendors
top_vendors = df_vendors_sorted[["Vendor ID", "Similarity_Score"]].head(10)

# Save to an Excel file
output_path = pd.ExcelFile("bhumika_output.xlsx")
top_vendors.to_excel(output_path, index=False)

print(f"Top 10 vendors saved to: {output_path}")

